# âš™ï¸ KEDA + Redis Worker Autoscaling Example

Este repositorio contiene un ejemplo prÃ¡ctico de cÃ³mo utilizar **KEDA (Kubernetes-based Event Driven Autoscaler)** para escalar automÃ¡ticamente un **worker en Python** segÃºn la cantidad de tareas pendientes en una cola Redis (`list`).

---

## ğŸ“¦ Componentes incluidos

- `redis.yaml`: despliega una instancia de Redis y su servicio.
- `redis-worker.yaml`: despliega el `Deployment` del worker Python.
- `keda-scaledobject.yaml`: define el `ScaledObject` de KEDA que escala el worker en funciÃ³n de la longitud de la cola Redis.
- `namespaces.yaml`: crea el namespace `keda`.
- `kustomization.yaml`: orquesta todos los recursos con Kustomize.
- `Dockerfile`: construye la imagen del worker Python.
- `worker.py`: script Python que escucha tareas desde Redis.

---

## ğŸš€ Requisitos

- Un clÃºster de Kubernetes (local con Minikube o en la nube)
- `kubectl` configurado y apuntando al clÃºster
- Acceso a internet
- [KEDA instalado](https://keda.sh/docs/)
- Docker para construir la imagen del worker

---

## ğŸ›  InstalaciÃ³n paso a paso

### 1. Instalar KEDA

```bash
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
helm install keda kedacore/keda   --version 2.17.2   --namespace keda   --create-namespace
```

### 2. ğŸ³ (Opcional) Construir tu propia imagen del worker

Si deseas modificar el script del `worker.py` y construir tu propia imagen, puedes usar el `Dockerfile` incluido:

```bash
docker build -t <tu-usuario>/redis-worker:latest .
docker push <tu-usuario>/redis-worker:latest
```

Luego modifica `redis-worker.yaml` para usar tu imagen personalizada.

ğŸ’¡ Por defecto, ya puedes usar la imagen preconstruida desde Docker Hub publicada por el autor de este ejemplo.

### 3. Aplicar los manifiestos con Kustomize

```bash
kubectl apply -k .
```

---

## ğŸ’¡ Â¿CÃ³mo funciona?

- El **script `worker.py`** escucha tareas de una lista Redis (`myqueue`) usando `redis-py`.
- **KEDA** monitorea la longitud de esa lista.
- Si hay mÃ¡s de 5 tareas en la cola, KEDA escalarÃ¡ el deployment del worker hasta 5 rÃ©plicas.
- Cuando la cola se vacÃ­a, reduce el nÃºmero de rÃ©plicas automÃ¡ticamente a 0.

---

## ğŸ§ª Pruebas y validaciÃ³n

### Enviar tareas a Redis

Ejecuta un shell en el pod de Redis:

```bash
kubectl exec -it deploy/redis -- sh
```

Desde ahÃ­, usa este script para empujar tareas:

```bash
i=1; while true; do echo "Enviando tareas del $i al $((i+39))"; for j in $(seq $i $((i+39))); do redis-cli -h redis rpush myqueue "task$j"; done; i=$((i+40)); sleep 10; done
```

### Observar el escalado

```bash
kubectl get pods -n keda
```

DeberÃ­as ver cÃ³mo los pods `redis-worker` aumentan o disminuyen segÃºn la carga.

---

## ğŸ“‚ Estructura del proyecto

```
.
â”œâ”€â”€ Dockerfile                         # Imagen del worker
â”œâ”€â”€ keda-scaledobject.yaml            # ScaledObject de KEDA
â”œâ”€â”€ kustomization.yaml                # Orquestador de recursos
â”œâ”€â”€ namespaces.yaml                   # Namespace para KEDA
â”œâ”€â”€ redis.yaml                        # Redis deployment + service
â”œâ”€â”€ redis-worker.yaml                 # Worker deployment
â”œâ”€â”€ redis-worker-script-configmap.yaml # Script del worker como ConfigMap
â”œâ”€â”€ worker.py                         # CÃ³digo Python del worker
â””â”€â”€ README.md
```

---

## ğŸ“– Recursos Ãºtiles

- [KEDA Documentation](https://keda.sh/)
- [Redis CLI](https://redis.io/docs/ui/cli/)
- [Kustomize](https://kustomize.io/)
- [Docker](https://docs.docker.com/)

---

Este ejemplo es perfecto para aprender cÃ³mo escalar automÃ¡ticamente cargas de trabajo en Kubernetes usando eventos de mensajerÃ­a, como una cola en Redis. TambiÃ©n puedes extender este patrÃ³n a otros sistemas como Kafka, RabbitMQ, Azure Queue, entre otros. âš¡ï¸